{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DT').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"sep\",\",\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"test2_score.txt\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()\n",
    "numeric_features = ['Userid','trackid','label','albumscore','artistscore','genreamax','genreamin','genreamean']\n",
    "\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# numeric_data = df.select(numeric_features).toPandas()\n",
    "\n",
    "# axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "# n = len(numeric_data.columns)\n",
    "# for i in range(n):\n",
    "#     v = axs[i, 0]\n",
    "#     v.yaxis.label.set_rotation(0)\n",
    "#     v.yaxis.label.set_ha(\"right\")\n",
    "#     v.set_yticks(())\n",
    "#     h = axs[n-1, i]\n",
    "#     h.xaxis.label.set_rotation(90)\n",
    "#     h.set_xticks(())\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.types as types\n",
    "df=df.withColumn('label',df['label'].cast(types.IntegerType()))\n",
    "df=df.withColumn('Userid',df['Userid'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackid',df['trackid'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumscore',df['albumscore'].cast(types.IntegerType()))\n",
    "df=df.withColumn('artistscore',df['artistscore'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamax',df['genreamax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamin',df['genreamin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamean',df['genreamean'].cast(types.IntegerType()))\n",
    "\n",
    "assembler=VectorAssembler(inputCols=['Userid','trackid','label','artistscore','genreamax','genreamin','genreamean'],outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(stages=[assembler])\n",
    "model=pipeline.fit(df)\n",
    "df=model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.95,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handleInvalid = \"keep\"\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label',maxDepth = 3,)\n",
    "dtModel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtModel.transform(test)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"sep\",\",\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"test_score_1.txt\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df1.take(5), columns=df1.columns).transpose()\n",
    "numeric_features = ['Userid','trackid','albumscore','artistscore','genreamax','genreamin','genreamean']\n",
    "\n",
    "df1.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.types as types\n",
    "# df1=df1.withColumn('label',df['label'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('Userid',df1['Userid'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('trackid',df1['trackid'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('albumscore',df1['albumscore'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('artistscore',df1['artistscore'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('genreamax',df1['genreamax'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('genreamin',df1['genreamin'].cast(types.IntegerType()))\n",
    "df1=df1.withColumn('genreamean',df1['genreamean'].cast(types.IntegerType()))\n",
    "\n",
    "assembler1=VectorAssembler(inputCols=['Userid','trackid','artistscore','genreamax','genreamin','genreamean'],outputCol='features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(stages=[assembler1])\n",
    "model1=pipeline.fit(df1)\n",
    "df1=model1.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = dtModel.transform(df1)\n",
    "predictions1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pre=predictions1.rdd.map(lambda x: x.prediction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=predictions1.rdd.map(lambda x: x.Userid).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track=predictions1.rdd.map(lambda x: x.trackid).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(120000):\n",
    "    output.append([user[i],track[i],pre[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"decisionTreeOutput.csv\", \"w\") as output_file:\n",
    "    \n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow([\"TrackID\", \"Predictor\"])\n",
    "    for line in output:\n",
    "        item = [str(line[0]) + \"_\" + str(line[1]), line[2]]\n",
    "        writer.writerow(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in output:\n",
    "    string = line[0] + \",\" +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
