{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DT').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Userid: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- trackid: integer (nullable = true)\n",
      " |-- albumscore: integer (nullable = true)\n",
      " |-- trackMax: integer (nullable = true)\n",
      " |-- trackMin: integer (nullable = true)\n",
      " |-- trackMean: double (nullable = true)\n",
      " |-- artistscore: integer (nullable = true)\n",
      " |-- albumMax: integer (nullable = true)\n",
      " |-- albumMin: integer (nullable = true)\n",
      " |-- albumMean: double (nullable = true)\n",
      " |-- genreamax: integer (nullable = true)\n",
      " |-- genreamin: integer (nullable = true)\n",
      " |-- genreamean: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"sep\",\",\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"test2_score_feature_final.txt\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Userid</th>\n",
       "      <td>6000</td>\n",
       "      <td>206286.491</td>\n",
       "      <td>3515.749299951101</td>\n",
       "      <td>200031</td>\n",
       "      <td>212234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackid</th>\n",
       "      <td>6000</td>\n",
       "      <td>149260.0025</td>\n",
       "      <td>86146.45891885243</td>\n",
       "      <td>65</td>\n",
       "      <td>296098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5000416718757232</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumscore</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackMax</th>\n",
       "      <td>6000</td>\n",
       "      <td>15.239833333333333</td>\n",
       "      <td>33.29398436461707</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackMin</th>\n",
       "      <td>6000</td>\n",
       "      <td>12.644166666666667</td>\n",
       "      <td>29.643880918634974</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackMean</th>\n",
       "      <td>6000</td>\n",
       "      <td>14.152771353128198</td>\n",
       "      <td>31.327078426246768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artistscore</th>\n",
       "      <td>6000</td>\n",
       "      <td>23.488666666666667</td>\n",
       "      <td>38.63689523198974</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumMax</th>\n",
       "      <td>6000</td>\n",
       "      <td>26.79083333333333</td>\n",
       "      <td>40.21001397052079</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumMin</th>\n",
       "      <td>6000</td>\n",
       "      <td>24.360833333333332</td>\n",
       "      <td>38.050965103350975</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumMean</th>\n",
       "      <td>6000</td>\n",
       "      <td>25.695232167568705</td>\n",
       "      <td>38.89176208967984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genreamax</th>\n",
       "      <td>6000</td>\n",
       "      <td>34.8425</td>\n",
       "      <td>40.726573667779995</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genreamin</th>\n",
       "      <td>6000</td>\n",
       "      <td>31.997</td>\n",
       "      <td>39.38594757623255</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genreamean</th>\n",
       "      <td>6000</td>\n",
       "      <td>33.41119444444444</td>\n",
       "      <td>39.5647285090302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0                   1                   2       3       4\n",
       "summary      count                mean              stddev     min     max\n",
       "Userid        6000          206286.491   3515.749299951101  200031  212234\n",
       "trackid       6000         149260.0025   86146.45891885243      65  296098\n",
       "label         6000                 0.5  0.5000416718757232       0       1\n",
       "albumscore    6000                 0.0                 0.0       0       0\n",
       "trackMax      6000  15.239833333333333   33.29398436461707       0     100\n",
       "trackMin      6000  12.644166666666667  29.643880918634974       0     100\n",
       "trackMean     6000  14.152771353128198  31.327078426246768     0.0   100.0\n",
       "artistscore   6000  23.488666666666667   38.63689523198974       0     100\n",
       "albumMax      6000   26.79083333333333   40.21001397052079       0     100\n",
       "albumMin      6000  24.360833333333332  38.050965103350975       0     100\n",
       "albumMean     6000  25.695232167568705   38.89176208967984     0.0   100.0\n",
       "genreamax     6000             34.8425  40.726573667779995       0     100\n",
       "genreamin     6000              31.997   39.38594757623255       0     100\n",
       "genreamean    6000   33.41119444444444    39.5647285090302     0.0   100.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()\n",
    "numeric_features = ['Userid','trackid','label','albumscore','trackMax','trackMin','trackMean','artistscore','albumMax','albumMin','albumMean','genreamax','genreamin','genreamean']\n",
    "\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.types as types\n",
    "df=df.withColumn('label',df['label'].cast(types.IntegerType()))\n",
    "df=df.withColumn('Userid',df['Userid'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackid',df['trackid'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumscore',df['albumscore'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackMax',df['trackMax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackMin',df['trackMin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackMean',df['trackMean'].cast(types.IntegerType()))\n",
    "df=df.withColumn('artistscore',df['artistscore'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumMax',df['albumMax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumMin',df['albumMin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumMean',df['albumMean'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamax',df['genreamax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamin',df['genreamin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamean',df['genreamean'].cast(types.IntegerType()))\n",
    "\n",
    "assembler=VectorAssembler(inputCols=['Userid','trackid','label','albumscore','trackMax','trackMin','trackMean','artistscore','albumMax','albumMin','albumMean','genreamax','genreamin','genreamean'],outputCol='features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(stages=[assembler])\n",
    "model=pipeline.fit(df)\n",
    "df=model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.95,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 5703\n",
      "Test Dataset Count: 297\n"
     ]
    }
   ],
   "source": [
    "handleInvalid = \"keep\"\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "rfModel = rf.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+----------+--------+--------+---------+-----------+--------+--------+---------+---------+---------+----------+--------------------+--------------------+--------------------+----------+\n",
      "|Userid|label|trackid|albumscore|trackMax|trackMin|trackMean|artistscore|albumMax|albumMin|albumMean|genreamax|genreamin|genreamean|            features|       rawPrediction|         probability|prediction|\n",
      "+------+-----+-------+----------+--------+--------+---------+-----------+--------+--------+---------+---------+---------+----------+--------------------+--------------------+--------------------+----------+\n",
      "|200031|    0| 130183|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1],[200031...|[9.40370181881176...|[0.94037018188117...|       0.0|\n",
      "|200031|    0| 198762|         0|       0|       0|        0|          0|       0|       0|        0|       90|       90|        90|(14,[0,1,11,12,13...|[9.16580016624963...|[0.91658001662496...|       0.0|\n",
      "|200118|    1|  63358|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1,2],[2001...|[2.40370181881175...|[0.24037018188117...|       1.0|\n",
      "|200143|    0| 187136|         0|       0|       0|        0|          0|       0|       0|        0|       20|       20|        20|(14,[0,1,11,12,13...|[9.40370181881176...|[0.94037018188117...|       0.0|\n",
      "|200160|    0| 293195|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1],[200160...|[9.69023759005805...|[0.96902375900580...|       0.0|\n",
      "|200279|    0|  58919|         0|       0|       0|        0|          0|       0|       0|        0|       80|       80|        80|(14,[0,1,11,12,13...|[9.16580016624963...|[0.91658001662496...|       0.0|\n",
      "|200442|    0| 171490|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1],[200442...|[9.40370181881176...|[0.94037018188117...|       0.0|\n",
      "|200463|    0| 156523|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1],[200463...|[9.40370181881176...|[0.94037018188117...|       0.0|\n",
      "|200497|    0| 231141|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1],[200497...|[9.69023759005805...|[0.96902375900580...|       0.0|\n",
      "|200500|    0|  23824|         0|       0|       0|        0|          0|       0|       0|        0|        0|        0|         0|(14,[0,1],[200500...|[9.40370181881176...|[0.94037018188117...|       0.0|\n",
      "+------+-----+-------+----------+--------+--------+---------+-----------+--------+--------+---------+---------+---------+----------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfModel.transform(test)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Userid: integer (nullable = true)\n",
      " |-- trackid: integer (nullable = true)\n",
      " |-- albumscore: integer (nullable = true)\n",
      " |-- trackMax: integer (nullable = true)\n",
      " |-- trackMin: integer (nullable = true)\n",
      " |-- trackMean: double (nullable = true)\n",
      " |-- artistscore: integer (nullable = true)\n",
      " |-- albumMax: integer (nullable = true)\n",
      " |-- albumMin: integer (nullable = true)\n",
      " |-- albumMean: double (nullable = true)\n",
      " |-- genreamax: integer (nullable = true)\n",
      " |-- genreamin: integer (nullable = true)\n",
      " |-- genreamean: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"sep\",\",\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"test_score_feature_final.txt\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Userid</th>\n",
       "      <td>120000</td>\n",
       "      <td>224372.7714</td>\n",
       "      <td>14155.612277030954</td>\n",
       "      <td>199810</td>\n",
       "      <td>249010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackid</th>\n",
       "      <td>120000</td>\n",
       "      <td>147871.92235833334</td>\n",
       "      <td>85421.19935293401</td>\n",
       "      <td>1</td>\n",
       "      <td>296099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumscore</th>\n",
       "      <td>120000</td>\n",
       "      <td>23.685075</td>\n",
       "      <td>38.7678462791902</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackMax</th>\n",
       "      <td>120000</td>\n",
       "      <td>14.660025</td>\n",
       "      <td>32.924395344452066</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackMin</th>\n",
       "      <td>120000</td>\n",
       "      <td>12.207208333333334</td>\n",
       "      <td>29.352143918452313</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trackMean</th>\n",
       "      <td>120000</td>\n",
       "      <td>13.612407135284117</td>\n",
       "      <td>30.990106545307405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artistscore</th>\n",
       "      <td>120000</td>\n",
       "      <td>35.016175</td>\n",
       "      <td>42.7330026359253</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumMax</th>\n",
       "      <td>120000</td>\n",
       "      <td>26.612633333333335</td>\n",
       "      <td>40.195132924332924</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumMin</th>\n",
       "      <td>120000</td>\n",
       "      <td>24.08203333333333</td>\n",
       "      <td>37.95270333713142</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albumMean</th>\n",
       "      <td>120000</td>\n",
       "      <td>25.473986883690248</td>\n",
       "      <td>38.8278736374371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genreamax</th>\n",
       "      <td>120000</td>\n",
       "      <td>35.471716666666666</td>\n",
       "      <td>40.80419698479329</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genreamin</th>\n",
       "      <td>120000</td>\n",
       "      <td>32.74715833333333</td>\n",
       "      <td>39.44356509895071</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genreamean</th>\n",
       "      <td>120000</td>\n",
       "      <td>34.14982360930737</td>\n",
       "      <td>39.70187105524824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0                   1                   2       3       4\n",
       "summary       count                mean              stddev     min     max\n",
       "Userid       120000         224372.7714  14155.612277030954  199810  249010\n",
       "trackid      120000  147871.92235833334   85421.19935293401       1  296099\n",
       "albumscore   120000           23.685075    38.7678462791902       0     100\n",
       "trackMax     120000           14.660025  32.924395344452066       0     100\n",
       "trackMin     120000  12.207208333333334  29.352143918452313       0     100\n",
       "trackMean    120000  13.612407135284117  30.990106545307405     0.0   100.0\n",
       "artistscore  120000           35.016175    42.7330026359253       0     100\n",
       "albumMax     120000  26.612633333333335  40.195132924332924       0     100\n",
       "albumMin     120000   24.08203333333333   37.95270333713142       0     100\n",
       "albumMean    120000  25.473986883690248    38.8278736374371     0.0   100.0\n",
       "genreamax    120000  35.471716666666666   40.80419698479329       0     100\n",
       "genreamin    120000   32.74715833333333   39.44356509895071       0     100\n",
       "genreamean   120000   34.14982360930737   39.70187105524824     0.0   100.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df1.take(5), columns=df1.columns).transpose()\n",
    "numeric_features = ['Userid','trackid','albumscore','trackMax','trackMin','trackMean','artistscore','albumMax','albumMin','albumMean','genreamax','genreamin','genreamean']\n",
    "\n",
    "df1.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.types as types\n",
    "#df=df.withColumn('label',df['label'].cast(types.IntegerType()))\n",
    "df=df.withColumn('Userid',df['Userid'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackid',df['trackid'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumscore',df['albumscore'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackMax',df['trackMax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackMin',df['trackMin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('trackMean',df['trackMean'].cast(types.IntegerType()))\n",
    "df=df.withColumn('artistscore',df['artistscore'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumMax',df['albumMax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumMin',df['albumMin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('albumMean',df['albumMean'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamax',df['genreamax'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamin',df['genreamin'].cast(types.IntegerType()))\n",
    "df=df.withColumn('genreamean',df['genreamean'].cast(types.IntegerType()))\n",
    "\n",
    "assembler1=VectorAssembler(inputCols=['Userid','trackid','albumscore','trackMax','trackMin','trackMean','artistscore','albumMax','albumMin','albumMean','genreamax','genreamin','genreamean'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline=Pipeline(stages=[assembler1])\n",
    "model1=pipeline.fit(df1)\n",
    "df1=model1.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+--------+--------+---------+-----------+--------+--------+---------+---------+---------+----------+--------------------+--------------------+--------------------+----------+\n",
      "|Userid|trackid|albumscore|trackMax|trackMin|trackMean|artistscore|albumMax|albumMin|albumMean|genreamax|genreamin|genreamean|            features|       rawPrediction|         probability|prediction|\n",
      "+------+-------+----------+--------+--------+---------+-----------+--------+--------+---------+---------+---------+----------+--------------------+--------------------+--------------------+----------+\n",
      "|199810| 208019|         0|       0|       0|      0.0|          0|       0|       0|      0.0|        0|        0|       0.0|(13,[0,1],[199810...|[9.62060771992818...|[0.96206077199281...|       0.0|\n",
      "|199810|  74139|         0|       0|       0|      0.0|          0|       0|       0|      0.0|       80|       80|      80.0|(13,[0,1,10,11,12...|          [10.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|199810|   9903|         0|       0|       0|      0.0|          0|       0|       0|      0.0|        0|        0|       0.0|(13,[0,1],[199810...|[9.40370181881176...|[0.94037018188117...|       0.0|\n",
      "|199810| 242681|         0|       0|       0|      0.0|          0|       0|       0|      0.0|        0|        0|       0.0|(13,[0,1],[199810...|[9.69023759005805...|[0.96902375900580...|       0.0|\n",
      "|199810|  18515|         0|       0|       0|      0.0|         70|       0|       0|      0.0|        0|        0|       0.0|(13,[0,1,6],[1998...|[8.12015503875969...|[0.81201550387596...|       0.0|\n",
      "|199810| 105760|         0|       0|       0|      0.0|         90|       0|       0|      0.0|       80|       80|      80.0|(13,[0,1,6,10,11,...|           [9.0,1.0]|           [0.9,0.1]|       0.0|\n",
      "|199812| 276940|         0|       0|       0|      0.0|          0|       0|       0|      0.0|        0|        0|       0.0|(13,[0,1],[199812...|[9.69023759005805...|[0.96902375900580...|       0.0|\n",
      "|199812| 142408|       100|       0|       0|      0.0|        100|     100|     100|    100.0|       80|       80|      80.0|[199812.0,142408....|          [0.0,10.0]|           [0.0,1.0]|       1.0|\n",
      "|199812| 130023|       100|       0|       0|      0.0|        100|     100|     100|    100.0|       80|       80|      80.0|[199812.0,130023....|          [0.0,10.0]|           [0.0,1.0]|       1.0|\n",
      "|199812|  29189|         0|       0|       0|      0.0|          0|       0|       0|      0.0|       80|       80|      80.0|(13,[0,1,10,11,12...|          [10.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "+------+-------+----------+--------+--------+---------+-----------+--------+--------+---------+---------+---------+----------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions1 = rfModel.transform(df1)\n",
    "predictions1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 59, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e81529ae8d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/627lyr/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/627lyr/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/627lyr/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/627lyr/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 59, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n"
     ]
    }
   ],
   "source": [
    "pre=predictions1.rdd.map(lambda x: x.prediction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
