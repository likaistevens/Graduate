{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data\n",
    "- Import BeautifulSoup from bs4, which is the module that can actually parse the HTML of the web page retrieved from the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import get to call a get request on the site\n",
    "from requests import get\n",
    "\n",
    "#get the first page of the east bay housing prices\n",
    "response = get('https://sfbay.craigslist.org/search/eby/apa?hasPic=1&availabilityMode=0') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the find_all method on the newly created html_soup variable in the code above, you can find the posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the screenshot below, you can see that it’s \n",
    "- <li class=“result-row”> \n",
    "- That is the tag for one single post,  \n",
    "- which is literally the box that contains all the elements you grabbed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(type(posts)) #to double check that I got a ResultSet\n",
    "print(len(posts)) #to double check I got 120 (elements/page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grab the first post and all the variables you want from it, make sure you know how to access each of them for one post before you loop the whole page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_one=posts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$2400\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the price of the first post\n",
    "\n",
    "\n",
    "post_one_price = post_one.a.text\n",
    "post_one_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$2400'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### remove white space from the string\n",
    "post_one_price.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I grabbed the date and time by specifying the attribute ‘datetime’ on class ‘result-date’. By specifying the ‘datetime’ attribute, I saved a step in data cleaning by making it unnecessary to convert this attribute from a string to a datetime object. This could also be made into a one-liner by placing ['datetime'] at the end of the .find() call, but I split it into two lines for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-02-27 07:18'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the time and datetime it was posted\n",
    "post_one_time = post_one.find('time', class_= 'result-date')\n",
    "post_one_datetime = post_one_time['datetime']\n",
    "post_one_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title is a and that class, link is grabbing the href attribute of that variable\n",
    "post_one_title = post_one.find('a', class_='result-title hdrlnk')\n",
    "post_one_link = post_one_title['href']\n",
    "\n",
    "#easy to grab the post title by taking the text element of the title variable\n",
    "post_one_title_text = post_one_title.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1br'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grabs the whole segment of housing details. We will need missing value handling in the loop as this kind of detail is not common in posts\n",
    "#the text can be split, and we can use indexing to grab the elements we want. number of bedrooms is the first element.\n",
    "#sqft is the third element\n",
    "\n",
    "post_one_num_bedrooms = post_one.find('span', class_ = 'housing').text.split()[0]\n",
    "post_one_num_bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'814'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "post_one_sqft = post_one.find('span', class_ = 'housing').text.split()[2][:-3] #cleans the ft2 at the end\n",
    "post_one_sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (berkeley)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "post_one_hood = posts[0].find('span', class_='result-hood').text #grabs the neighborhood, this is the problem column that requires\n",
    "#a lot of cleaning and figuring out later.\n",
    "post_one_hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build out the loop\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully!\n",
      "Page 2 scraped successfully!\n",
      "Page 3 scraped successfully!\n",
      "Page 4 scraped successfully!\n",
      "Page 5 scraped successfully!\n",
      "Page 6 scraped successfully!\n",
      "Page 7 scraped successfully!\n",
      "Page 8 scraped successfully!\n",
      "Page 9 scraped successfully!\n",
      "Page 10 scraped successfully!\n",
      "Page 11 scraped successfully!\n",
      "Page 12 scraped successfully!\n",
      "Page 13 scraped successfully!\n",
      "Page 14 scraped successfully!\n",
      "Page 15 scraped successfully!\n",
      "Page 16 scraped successfully!\n",
      "Page 17 scraped successfully!\n",
      "Page 18 scraped successfully!\n",
      "Page 19 scraped successfully!\n",
      "Page 20 scraped successfully!\n",
      "Page 21 scraped successfully!\n",
      "Page 22 scraped successfully!\n",
      "Page 23 scraped successfully!\n",
      "Page 24 scraped successfully!\n",
      "Page 25 scraped successfully!\n",
      "Page 26 scraped successfully!\n",
      "\n",
      "\n",
      "Scrape complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get(\"https://sfbay.craigslist.org/search/eby/apt?\" \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&hasPic=1\"\n",
    "                   + \"&availabilityMode=0\")\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "            #    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "            #    sqft = np.nan\n",
    "            #    sqfts.append(sqft)\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
